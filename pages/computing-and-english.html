<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: 'Courier New', monospace;
        }
        .heading {
            display: flex;
            gap: 50px;
            justify-content: space-between;
            margin-bottom: 75px;
        }
        .title {
            font-size: 75px;
            font-style: italic;
            font-weight: 100;
            margin: 0;
            text-decoration: none;
        }
        h2 {
            font-size: 85px;
            font-weight: 100;
        }
        p {
            line-height: 2;
        }
        #logo {
            height: 85px;
        }
        .image-note {
            line-height: 1;
        }
        .content-image-box {
            display: flex;
            justify-content: center;
        }
        .image-note-link {
            display: flex;
            justify-content: center;
        }
    </style>
    <title>arbitrary node</title>
</head>
<body>
    <div class='heading'>
        <a href='../index.html' class='title'>arbitrary node_</a>
        <img src='../images/logo.png' id='logo'>
    </div>
    <h2>computing and english_</h2>
    <h3>abstract</h3>
    <p>English is the dominant language of today’s global world. It is the premier language of science, technology, navigation, international law, and international communication more generally. Particularly in computing, it occupies a unique place as not only the primary medium of expert communication, but as an underlying substrate upon which most programming languages and standards are built. In this paper we investigate just how this came to be, how the language reinforces itself over time in the realm of computing, how non-native speakers or non-speakers of English fare in the field, and what, if anything, can be done to narrow the problem. We conclude that the linguistic reality of computing presents significant challenges for learners who do not speak English natively, and that the problem resides mainly in meager attempts at translation usually only at the standard level. To propose a partial solution, we argue that a modest yet comprehensive attempt at online source code level translation and interlingual program compilation / interpretation undergirded by a system of wiki-style surjective keyword mapping and standardization would provide programmers who are nonnative to English with an invaluable tool for tying local semantic meaning to internationally standardized programming keywords, while yielding accurate translation and preserving the idiom.</p>
    <h3>introduction</h3>
    <P>We live in an increasingly globalized world where people thousands of miles apart need to and want to talk to each other more often than they perhaps ever have, and where automatic systems operate in unison across the globe. Despite the recently predominant cultural pluralism abundant here in the west, there is still the need to communicate on grounds established not necessarily by what is best for everyone on average now, but by what the trends of cultural domination and prosperity have determined in the past. The trends of the last 200 years, via the colonial and maritime hegemony of the British Empire, and via the cultural, economic, and military hegemony of the United States have put formal American and British English at the global forefront of science, technology, navigation, law, etc. (Ammon & Kaplan, 2001, pp. 5-10) Computer Science, or Computing more generally, and its associated mathematics, have strong ties to an English tradition: Though various mechanical calculators and underlying mathematics were invented by continental savants who doubtlessly helped lead to the development of a bona fide Computer Science in the 20th century (O'Regan, 2021, pp. 35-40) and though Kurt Gödel’s theories on the limits of computability underpin theoretical work in the field, (Schmidhuber, 2021) the vast majority of Computer Science is rooted in mathematical theories and technological innovations put forward and made in either the US or the UK in the late 19th and early 20th centuries.</P>
    <p>Naturally then, much of the early literature of CS was written in English, but when the use of and participation in the discipline eventually spread beyond its origin in the Anglosphere, a problem arose: CS isn’t like the natural sciences, which describe and deal with objects that are easy to point to as phenomena in the real world, nor is it in this sense like engineering, which despite relying on standardization, can function largely independently of natural language so long as a system of units is established — CS deals primarily with logical systems which are foundationally artificial, and thus in this case foundationally based on natural language. These are programming languages which are standardized to a specification using keywords borrowed from a particular natural language, or character encodings based on a particular alphabet, or even documentation sets of any given programming language, generated by inline comments in the code itself, and so written by speakers of the same language used to base the programming language being written about.</p>
    <p>When the designer of a software system has to cater their product to a particular locale (Language, format, etc.), but still has to ensure that their work conforms to a set of CS-level standards, they run into problems of what are called internationalization and localization. Internationalization in this case refers to the ability of software to be adaptable in use cases spanning many different locales, as it would be both ‘legal’ (That is, it can run) in different possible interpretation environments, and assuredly bug-free in those environments. Localization in this case is referring to the extent to which software, when used in a particular locale, conforms to certain expectations and standards within that locale. For example, dates are formatted differently across the globe and the calendars used for those dates may vary, colors that elicit a feeling in one culture may not in another, currency is different for almost every country, and things like telephone numbers, addresses, etc. are all free to be formatted completely differently from place to place.</p>
    <p>This problem is perhaps most notorious when it comes to the ways in which languages vary, because when reinterpreting the meaning of a function in a software system in another natural language, not only is there a different vocabulary to learn to use; the meaning a vocabulary allows may not be a one to one relationship from the starting language, and the way the language is expressed might be very different compared to what is standard in the Anglosphere. For example, Hebrew is read right to left, and so software designers might want to ensure that Hebrew text appearing in their software system can be translated to the other side of the text field containing it without breaking the look or feel. And what of Chinese? How might the myriad different logographs of Chinese script be encoded into a computer system? These are the sorts of engineering challenges that software developers have had to deal with over the years.</p>
    <p>It is through this reality that we come to our first main hypothesis: That the further away a language is from English, both orthographically and grammatically, the harder a time a native speaker of that language will have trying to program using a system based on English. In this case, a speaker of Frisian and Dutch will have a much easier time programming than a speaker of a Semitic language, for example. This is not only because of the level of ‘foreignness’ that English in this case would present, but also because the way in which English works grammatically and especially syntactically has had a definite effect on the structure of mainstream programming languages, meaning natives of languages dissimilar to English will find learning programming languages based on English inherently more foreign than otherwise. Abbott (1983) illustrates the formative relationship between English and mainstream programming clearly by showing how a simple procedure can translate English into a programming language like Ada. Incidentally, it is interesting to note that Ada, a popular object oriented programming used for the design of embedded systems, was created by a team led by French Computer Scientist Jean Ichbiah under contract by the United States Department of Defense. (The Ada Programming Language, n.d.) There is plenty of talent in the field outside of the Anglosphere, but the uniform medium of standard design remains to be English even among that pool of talent, especially when that talent is commissioned to further Computing development in the core Anglosphere in an increasingly interconnected world, where that talent isn’t tied to opportunity in their home country / language for reasons of technological or political limitation. When an American company outsources their software development, for example, this is what’s happening.</p>
    <p>There have been successful and unsuccessful attempts to make Computing more accessible to non-English speakers. Among the most successful innovations, which has been revolutionary to the development of a truly ‘world-wide’ web, has been Unicode. Unicode was a character encoding introduced in 1991 that unshackled mainstream computers from relying on ASCII, a 7 bit or 8-bit American-centered character encoding system that reproduced the English version of the Latin alphabet, ten digits, and a few extra symbols, including the American dollar symbol. (Encyclopædia Britannica, 1998) This means that before Unicode, if someone wanted to digitally print a character not in ASCII, such as a latin character with a diacritic, or a letter of any other alphabet, you’d have to design a computer’s operating system around a different, and oftentimes proprietary character encoding system which may be designed to provide one but not multiple of the character sets ASCII leaves out. (What is Unicode?, n.d.) Unicode came into being when the nascent internet was in development — if HTTP documents written in different languages were to be shared across the world, available for everyone to see, there needed to be some way that all those different characters could be viewed by everyone. (Fyfe, 2019) Unlike the 7 or 8 bit base of ASCII, which only renders the scheme either 128 or 256 possible combinations per character, Unicode is usually implemented in a 16-bit, or 2 byte per character form. This renders a little over 65000 options per character, which is enough to encode practically any alphabetic character ever produced by mankind. Unicode can also implement an extension mechanism to bring the amount of possible characters up into the millions via the use of ‘surrogate characters’, which allow for the encoding of non-alphabetic scripts which would otherwise overload the 65000 character capacity (Unicode Character Encoding, n.d.) — this means that Chinese text can be rendered anywhere on Earth, so long as Unicode can be interpreted on the machine in question. We take this kind of thing for granted nowadays, but this was no trivial feat of software engineering, especially when we consider the scale of the problem domain.</p>
    <p>This revolution in character encoding technology enabled anyone on Earth to produce and view their own written information on the web, and share it with anyone — a reality that opened the door for a huge number of people that were shut out of many of the infrastructural benefits that computing had been providing but that were locked behind an English & Western centric encoding scheme. (Uma & Suseela, 2014, pp. 40-54) But despite the massive improvement, at least in underlying technology, this was not close to enough to produce a sufficient linguistic pluralism in Computing — standards were still being based in English, and online resources remained consistently published in English. Later in the paper, we will discuss other and further past attempts at internationalization of the discipline, as well as reasons for their invariantly low levels of success. To begin though, we must look at what is actually happening when one language begins to dominate others in a domain of discourse.</p>
    <h3>lingua francas</h3>
    <p>The study of the interaction between speakers of different languages is called Contact Linguistics. Whenever speakers of separate linguistic groups want to communicate, but cannot due to a lack of mutual intelligibility and a lack of translatability, a dilemma arises — the participants of this hypothetical ‘conversation’, as we may abstractly call it, have essentially two options: They can A), ‘pidginize’ one or both of the languages of the participants and / or make use of signed deixis to refer to concrete objects in the real world, (Bakker et al., 1994, pp. 28-30) or B), one of the participants, be it a group or an individual, can decide to invest time and effort into learning the language of the other member(s) of the conversation. The process by which this typically occurs and the subsequent effect the new speakers of the ‘host’ language have on that language can be quite complicated, especially if that host language isn’t centrally standardized by a prescriptive grammar. We will not dive into this here, but it is important to note for our purposes that it is thought that oftentimes one of the effects an influx of new speakers speaking the language ‘imperfectly’ can have on the host language is that the host language’s grammar often moves away from a relatively highly inflected state to a more syntactically complicated state which can be expanded to match the previously higher level of linguistic displacement with a larger number of ‘helper words’. (Loss of Inflection, n.d.) Coincidentally, a good example of this is English itself: Old English, the earliest form of English that would have been spoken from around the time that the Anglo-Saxons first came to Britain en masse to around the time that the Normans invaded England in 1066, was, as attested in writings throughout that period, highly inflected compared to modern English: Nouns were declined in five cases, three genders, and two numbers, adjectives agreed with nouns in those three ways, and verbs were conjugated for three persons, two tenses, and three moods. (Quirk & Wrenn, 1957, pp. 19-40) In modern English though, we only inflect nouns for number and possession, and verbs are only inflected for tense and aspect.</p>
    <p>A ‘lingua franca’ is a language that has gained enough significance in a large region / the world as a language spoken to bridge the communication gap between speakers who do not share the same native language within that region. The existence of a lingua franca points to a high degree of the kind of second-language acquisition just mentioned, and the term itself, Italian for “Frankish tongue”, is historical in origin: After the Crusades had begun to take place in the Middle Ages, westerners, or Latins of various linguistic groups that had drifted away from a vulgar latin and were now speaking mutually unintelligible vernaculars, began to use a kind of southern Mediterranean trade pidgin, which was mostly a combination of middle French and Italian. (Mufwene, 2010) This political / economic origin also illustrates an important point about the development of a language into a lingua franca: the language to be acquired by other members of the ‘conversation’ in order for communication to take place is virtually never chosen arbitrarily. The choice is often based in a political or economic reality, such as a language being made a state language by a government, or such as a rich linguistic community trading with its neighbors. Or, a language may be chosen because it may simply be more prevalent than the other(s). Dutch sociologist Abram De Swaan formulated an effective way of describing the communicative potential of a language, which in theory determines the value that language has in the eyes of people ‘in the market’ for a second language. He calls it the Q-value, and it is defined as:</p>
    <P>Q<sub>i</sub> = (P<sub>i</sub> / N<sup>s</sup>) x (C<sub>i</sub> / M<sup>s</sup>)</P>
    <blockquote>“The utility of a language, i, for a given speaker in a constellation or sub-constellation, S, can be expressed in terms of its ‘communication value,’ Qi , which indicates its potential to link this speaker with other speakers in S. The ‘prevalence,’ pi , of language i refers to the number of speakers, Pi , who are competent in i, divided by all the speakers, NS, in constellation S. ‘Centrality,’ ci , refers to the number of multilingual speakers, Ci , who speak language i, divided by all the multilingual speakers in constellation S, MS. The communication or Q-value equals the product of the prevalence (pi) and the centrality (ci) of language i in constellation S.”</blockquote>
    <p>(Coupland & Swaan, 2013, p. 58)</p>
    <p>As de Swaan states, this is simply a measure of the value of acquiring a language based on that language’s ‘prevalence’ and ‘centrality’, meaning it says nothing about the value of learning any given language for religious, academic, or social reasons, etc. But even without factoring in real world considerations, it is easy to see how, by the consequence of a simple positive feedback loop, certain languages that initially only have a marginal advantage in this way can balloon their number of speakers very quickly in an environment where many people who cannot communicate with one another suddenly want to do so.</p>
    <p>Importantly, languages can also become prevalent because of a ‘technological’ innovation they make or introduce. We’ll quickly gloss over a few historical examples to illustrate how this happens:</p>
    <p>Sumerian, an ancient Mesopotamian language, was the first that we know of to make use of writing. (Woods et al., 2010, pp. 33-85) The oldest surviving examples that we have were produced around 3500 BC, and are examples that straddle the line between what would be referred to as ‘proto-writing’, at least in ancient Mesopotamia, and ‘cuneiform’, the writing system that revolutionized information displacement, mobility, and permanence by moving it outside of speakers’ minds and thus changing the entire western Eurasian world — by imprinting pictographs representing commodities and other basic concepts into wet clay with reed styluses, the Sumerians were able to keep records more easily, express ideas more concisely, and enshrine law more completely, just to begin to name a few of the things the transition to a literate society allowed for.</p>
    <div class='content-image-box'>
        <img src='../images/computing-and-english/cuneiform.png'>
    </div>
    <p class='image-note content-image-box'>Figure 1.<br>Logographic cuneiform, tablet and reed stylus<br>Note. From Getty Images. [Illustration] by D. Kindersley</p>
    <a href='https://www.gettyimages.com/detail/illustration/illustration-of-using-reed-pen-to-write-royalty-free-illustration/98193130' class='image-note-link'>Link</a>
    <p>Gradually, these pictographs developed into more efficiently written logographs (See Figure 1.), trading detail for minimalism. After a while, the characters that represented concepts no longer looked much like the things they originally represented, and more like arbitrary characters, shaped in development by the technology of writing that the Sumerians had. Sumer’s neighbors, in particular the Akkadians, adopted the logographic script, attaching their own words to the Sumerians’ concepts. The cuneiform writing system remained common in the ancient near east for thousands of years after its first appearance, thereby keeping Sumerian read and spoken by the learned as a classical language for a long time.</p>
    <p>Another example is that of Phonecian. Though the precise origins of the characteristic Phonecian writing system remain unknown, and thought to be from an obscure Proto-Canaanite language from around the 18th century BC, (Woodard & Hackett, 2008, pp. 82-103) the Phonecians were certainly the first people to use it to promote their language as the lingua franca of Bronze Age maritime trade and thereby spread the writing system, famously, to Greece, where it was used it to form the writing system which subsequently influenced virtually every western script afterwards. That innovation in writing was of course the alphabet. The switch from a logographic writing system, where there are tons of individual characters to be memorized which each represent a concept, to an alphabetic writing system, where there are only a few individual characters that each only represent a sound, allowed many more people to become literate, thereby increasing economic and intellectual productivity in alphabetically literate societies. (McLuhan & Logan, 1977, pp. 373-383) This was a plainly technological innovation, and it helped promote Phoenecian as a premier language despite the tongue itself not having anything about it that would make it inherently better than other languages.</p>
    <p>The relationship between English and Computing has produced the exact same effect that these ancient languages had with their innovations in writing. Many languages have borrowed what are called ‘loanwords’ from English to refer to computer-related items and concepts. For example, in Japanese, ‘computer’ is コンピュー, or konpyuutaa, ‘download’ is ダウンロード, or daunrōdo, and ‘link’ (As in, a hyperlink) is リンク, or rinku. In German, English loans can be seen in downloaden, booten, and crashen. And in Polish, words like dżojstik, kartridż, and interfejs refer to ‘joystick’, ‘cartridge’, and ‘interface’. There are tons of examples of this, and in general, the use of the internet has exacerbated the dominance of English in the global linguistic makeup by speeding up the transfer of information and centralizing American media culture online. (Kolko et al., 2000, pp. 155-158) If we think back to the Q-value theory put forward by de Swaan, we can think of the internet as not only being a product of English dominance, but as a vehicle by which the language acquisition feeding its growth is sped up exponentially, driving more and more people to learn English as a second language over a shorter period of time.</p>
    <h3>the ascent of english</h3>
    <p>English has not always been a language at the forefront of global communication, nor has it always been a lingua franca. For hundreds of years, English was a minority Germanic language spoken by merely a portion of the residents of the island of Britain, a harmless insular community even when taken as a whole. And it wasn’t until the late 11th century that an even remotely recognizable English began to form from the combination of Norman French and Old English.</p>
    <p>When the Normans invaded England in 1066, they brought with them a new prestigious language to be spoken on the island which was rooted in Latin. For a while, the Norman elite in England continued to speak their dialect of French, but as the Norman English monarchy gradually lost its ties to the mainland, it re-anglicized, reverting to the now simplified 'Middle' English, while inserting into it a spate of new words for concepts of government, art, and emotive expression. Many of these terms, or their derivatives, we still use today when we're trying to sound 'fancy' or formal — when we think of words in Modern English which have etymological roots in Latin, this is typically where they came from. (Solodow, 2010)</p>
    <p>After entering the renaissance, English had joined the pantheon of state-sponsored European tongues which were prominent within their government's domains and prestigious to the extent that a scientific community was beginning to write in them, but that were not yet European lingua francas — At this time in the early modern period, Latin was still the primary trade language / lingua franca in Europe, despite having already been a 'dead language' (Meaning, it has no native speakers) for hundreds of years. (Samarin, 1977)</p>
    <p>By the 18th century, French had stood out as the ‘vernacular’ which would overtake Latin (Weber, 2004) This was due in large part to the highly successful and long reign of Louis XIV, during which France became the most populous, powerful, and economically successful country in Europe. (Language and Diplomacy, 2011) If you were an educated European from the late 18th to the early 20th century, you probably knew French. Of course, at this point the distinct European linguistic pluralism the subcontinent maintains today had begun to take shape, but French was the preeminent language of the bunch due mainly to the value gained by its number of speakers, and by the political power its government could wield.</p>
    <p>After the battle of Trafalgar near the end of the Napoleonic wars in 1805, it was obvious that Britain had ensured hegemony in terms of maritime power. That hegemony would not be challenged until the industrial might of Germany would be unleashed in the opening stages of WW1, and so the period, during which Britain reached her greatest imperial extent, is often called ‘Pax Britannica’, roughly Latin for “The Peace of Britain” — a term modelled after the analogous historical term ‘Pax Romana’.</p>
    <div class='content-image-box'>
        <img src='../images/computing-and-english/empire.png'>
    </div>
    <p class='image-note content-image-box'>Figure 2.<br>The British Empire at its greatest territorial extent<br>Note. From Wikimedia Commons. by Vadac.</p>
    <a href='https://upload.wikimedia.org/wikipedia/commons/b/b8/British_Empire_1921.png' class='image-note-link'>Link</a>
    <p>It was during this time of unprecedented global success for the British that the English language saw its first real thrust onto the global stage as a lingua franca. (Porter, 2001) The language of prestige academia was perhaps still French during this period, but the nominal world domination of the British Empire and thus the increasing importance of its language for economic and political reasons meant that it was only a matter of time before English supplanted French. For example, by the time of the end of WW1, the Treaty of Versailles was written in both English and French, putting English on the same level as French as a language of diplomacy. This was a sign of the times back then. (Treaty of Versailles Protocol, 2020)</p>
    <p>As WW2 exhausted the now antiquated British empire, the United States, also victorious but uniquely untouched by the violence of the war, was poised to set the terms for the reconstruction of a new world order, in which they would take the baton from Britain and use their newfound position at the top of the world to continue to spread the English language in a world now connected like never before. Rather than doing so by founding colonies, or by pushing down on foreign governments to change their linguistic policies, the US continued to forge a path for the linguistic trend of English dominance by creating an economic dominion through which the trade language became English by definition. (Paul et al., 2004, p. 123) At this point, German was in no position to put up a fight, as its speakers found themselves ‘in timeout’, French had no real ability to expand due to its crushing capitulation in the war, Italian found itself in a similar situation to German, and other prospective European languages’ states were simply not prominent enough.</p>
    <p>It is through this global economic empire the US establishes that computing spreads as the technological medium of the new world order. Without the US, computing would have undoubtedly still developed, probably in a similar fashion, but through the US it did so with a great deal of freedom and incentive for rapid development. In any case, through computing, English was allowed to slingshot itself into a position no other language can or could ever really claim to have been in. As de Swaan notes,</p>
    <blockquote>In the course of the twentieth century, English has become the hypercentral
        language of the world language system. Even if there are languages with more
        speakers, such as (probably) Mandarin and Hindi, English remains the most
        central one, on account of the many multilinguals who have it in their repertoire.
        This has nothing to do with the intrinsic characteristics of the English language;
        on the contrary, its orthography and pronunciation make it quite unsuitable as a
        world language. It is a consequence of the particular history of the English-
        speaking nations and of reciprocal expectations and predictions about the lan-
        guage choices that prospective learners across the world will make. Even if the
        hegemonic position of the US were to decline, English would continue to be the
        hub of the world language system for quite some time, if only because so many
        millions of people have invested so much effort in learning it and for that very
        reason expect so many millions of other speakers to continue to use it.
        </blockquote>
    <p>(Coupland & Swaan, 2013, pp. 72-73)</p>
    <h3>the development of computer science</h3>
    <p>Computing, as it may be thought to exist before the end of the 19th century or early 20th century, comes ultimately from different areas of mathematics which each undergird the discipline in its practical application today, but that were not, until Boole, born in English. Attested arithmetic thinking goes, as far as we know, at least as far back as the Sumerians, but we aren’t really sure what sort of manual method they employed to carry out their calculations. They utiilized a sexegesimal, or base 60 numbering system, and recorded questions about problems such as how many ‘granaries’ of grain to share between a given number of men such that each man receives a certain number of silá of grain. (Chabert et al., 1999, p. 10) Chabert et al. also describes how the Babylonians, utilizing the cuneiform counting method invented by the Sumerians, but by using different symbols and an additive system, particular with a ones’ logogram which can be effectively ‘stacked’, developed and recorded an algorithm for finding inverses. From there on out, prominent developed civilizations each had their own ways of recording and talking about algorithmic arithmetic problems. (Chabert et al., 1999, pp. 8-20)</p>
    <p>The more specific mathematical roots of Computer Science have their origin in Gottfried Wilhelm Leibniz and George Boole. (O'Regan, 2021) Leibniz is perhaps best known for his independent co-invention of calculus with Isaac Newton and his philosophical treatise Theodicy, but he importantly also gave us both the ‘Stepped Reckoner’, the first digital mechanical calculator to be able to perform all four basic arithmetic operations (The first mechanical calculator, invented by Blaise Pascal, could only do addition and subtraction), and he helped develop a theory of binary numbers, which we now use as the digital basis for machine language in modern computers. As an illustrative side note, the paper in which Leibniz explained the system of binary integers, Explication de l'Arithmétique Binaire, authored in 1703, was written in French, which as discussed earlier had become the primary lingua franca of science in Europe by the 18th century.</p>
    <div class='content-image-box'>
        <img src='../images/computing-and-english/leibniz.png'>
    </div>
    <p class='image-note content-image-box'>Figure 3.<br>Excerpt from Leibniz’s Explication de l'Arithmétique Binaire<br>Note. from HAL archives</p>
    <a href='https://hal.archives-ouvertes.fr/ads-00104781/document' class='image-note-link'>Link</a>
    <p>Boole, as anyone who’s either ever programmed in a typed programming language or who has taken an introductory computing course can probably guess, was the inventor ‘Boolean Algebra’, a simple yet powerful logical framework which renders a particular type, the ‘boolean’, which is a value that can only either be TRUE or FALSE. The crucial and simple insight from here is to connect the boolean to the binary number system. This method, using the binary numbers in a system of boolean logic, is the fundamental basis of computation in computers (That is, computing machines) today. (O'Regan, 2021, p. 47)</p>
    <p>Boole’s mathematical innovations provide the first evidence for an origin of theoretical computing in an English tradition. And an English contemporary of Boole’s, Charles Babbage, would be the first to grapple with the idea of a programmable computing machine, the sort which would go on to change the world in the 20th century. (O'Regan, 2021, pp. 40-44) Babbage’s first breakthrough was with what he called the ‘Difference Engine’ — the goal with the Difference Engine was to create a mechanical calculator more advanced than the kinds invented by Pascal and Leibniz, which could produce accurate mathematical tables free from human error by being able to compute polynomials up to the fourth degree on 15 digit numbers, and by being able to compute logarithmic and trigonometric functions. While Babbage was able to produce prototypes for parts of the Difference Engine, he was never able to complete the machine himself, mainly due to the immense mechanical complexity the machine demanded relative to the metallurgical capabilities of the era. In 1842, the British government grew impatient and cut funding for his project, but in 1853, with funding from their government, two Swedish engineers would build the first working Difference Engine, based on Babbage’s design. The machine worked as Babbage intended, and presented Europe with a valuable tool for scientists who needed reliably accurate mathematical tables.</p>
    <p>In 1849 Babbage designed an improved version of the Difference Engine which could operate on 7th order differences and 31 digit numbers. This machine was never built in his time, but in 1991, one was built to the specification of Babbage’s design to commemorate the 200th anniversary of his birth, and it worked virtually flawlessly, in a true testament to his genius.</p>
    <p>But while working on the Difference Engine in the 1830s, Babbage was already looking ahead — he realized that the limitations of the Difference Engine’s capabilities could be overcome by designing a machine which would be able to execute all tasks that could be expressed in an algebraic notation, which for his purposes meant any ‘program’ that could be designed to be fed to this hypothetical machine on a series of punch cards. This was the first time anyone had envisioned designing something akin to a mechanical calculator which could be programmed. The machine worked by reading in ‘variable cards’ and ‘operation cards’, working on both, and by being able to store intermediate results in a way similar to modern computer memory. While the design of the machine was far beyond the realistic manufacturing capabilities of its age, and thus was never built, it represented a huge milestone in the development of computing, roughly 100 years before the work of Alan Turing.</p>
    <p>Thus, even before the turn of the 20th century, we can see how the economic power of the British empire was able to provide funding for far flung scientific projects which helped to drive the development of computing, even if those projects didn’t always produce useful results. At around the same time, in the US, the groundwork for development in American computing was also being laid — throughout the latter half of the 19th century, there had been a huge influx of European immigrants into the US, and so the population of the country had ballooned massively. (US Census Bureau Publications - census of Population and Housing, 2011) The 1880 census had taken 8 years to complete via the traditional counting methods employed, and since a census has to be taken every 10 years, this was a significant problem — as the population continued to grow, the US government sought the invention of a counting machine to aid in the inevitably more difficult count of the upcoming 1890 census. (Anderson, 2015, p. 102) Herman Hollerith, who had been working on the laborious 1880 census, designed a machine that made use of punched cards, similar to Babbage’s Analytical Engine, but instead of feeding in operations and variables, the punched cards would feed in data. Thus the Tabulating Machine, the winning design for use in the 1890 census, did not perhaps resemble modern computers as closely as the Analytical Engine did, but it was successfully built and used to great effect, thereby pioneering the standard for computing I/O when general purpose computers did eventually become feasible. As a side note, the company that Hollerith created to build the machine he patented, the Computing-Tabulating-Recording Company, or CTR, eventually changed its name to IBM, or International Business Machines, which is an American company recognizable today for its incalculable influence on computing. (Cruz, 2001) Thus by extension, Hollerith can be thought of as the founder of American business computing.</p>
    <p>These sorts of primitive computing machines continued to become more commonly used in both the US and the UK through the first half of the 20th century. This time period, spurred on by important developments in mathematics and the need for complex computation in the face of two world wars, saw the true beginnings of computer science.</p>
    <p>The first big development was that of American mathematician Alonso Church's lambda calculus in 1936. A formal system of mathematical logic, the lambda calculus allows for the abstraction of any kind of computation using a method which substitutes functions for variables and nests them in a systematic way. (Lambda calculus, 2021) While not explanatory of computation in and of itself, and perhaps seemingly trivial at first glance, the lambda calculus has been incredibly influential in programming language theory, where it has influenced many languages which are based on modelling instructions off of function abstraction, as well as languages which incorporate lambda calculus directly to support the creation of anonymous functions. LISP is perhaps the most famous example of this class of ‘functional’ programming languages, (Lisp (programming language), 2021) and you can find support for lambda expressions in virtually every popular imperative programming language nowadays.</p>
    <p>The other, certainly more important development was the invention of the idea of what would become known as the 'Turing Machine'. Alan Turing, a young and inspired English mathematician working at Cambridge, published in 1937 a landmark paper called "On Computable Numbers, with an Application to the Entscheidungsproblem", in which he reformulated Gödel's work on the limits of logical computability in terms of an abstract machine with rules for certain symbols working along an infinite "tape", while showing, famously, that there are some mathematical questions to which the answer is yes / no which have no algorithm that can reliably solve them for all input cases. (Turing, 1937) This paper has come to be thought of as the foundation stone of the discipline of Computer Science, as it gives a way to classify computing devices as being either able to compute everything computable given the right program, that is, as being 'Turing-complete', or as being unable to do so, and therefore as not technically being in the class of general computing devices. (Wigderson, 2019)</p>
    <p>As we can see, the most important and rapid developments in computing were occurring chiefly in the English speaking world. They did not determine that CS literature be written in English, but they did show how the learned west had shifted to using English as its lingua franca at this point, and if we factor in the apex of the British Empire around this time and the insulation of Germany into a militant, anti-liberal fascist state with the shrinking of global and European French influence, we can see how even outside of the fact that these developments in CS just happened to occur in the Anglosphere, the language in and of itself was moving with its own weight at this point, thus ensuring that any pioneering science would be done in it. And if we factor in the economic reality of the 20th century, we can see how it was unsurprising that the first real, general purpose computers were produced in the English speaking world (ENIAC, 2021), and thus how CS-level standards came to be based in English.</p>
    <h3>learning in english nonnatively</h3>
    <p>In order to determine the extent to which learning CS in English is harder for nonnative speakers of English compared to native English speakers, we should first look at the typical effects learning in English nonnatively has on students. For most learners, this will probably be their second language, and it will have probably been acquired during their secondary school years. This age is important, as we will shortly see.</p>
    <p>It is important to note the sheer number of people across the world nowadays who speak English as a second (or third, etc.) language. As of 2008 even, it was estimated that the total number of nonnative English speakers dwarfed the number of native speakers probably by multiple times. (Crystal, n.d.) A consequence of this has been that many secondary students, especially those in countries whose national language is not a lingua franca to some extent, will have their instruction switched over to English once either their ability in the language is sufficient enough, and / or when the content of their schooling becomes economically or globally oriented enough to where they would need to know English in order to succeed in the field for which they’re developing skills. (Kaplan, 2001)</p>
    <p>It is not simply that since we typically grow up using one language versus another that we tend to remain most proficient in our native tongue; the way in which we acquire a language when we’re young is fundamentally different compared to how we acquire one when we’re older, (Kerschen & Matrinez, n.d.) and once we have that base, native language, it can be hard not to think in another language in terms of the first language, thereby hindering acquisition of fluid thinking in the second language.</p>
    <p>Language is, fundamentally speaking, a tool. In this sense, language is both a tool of communication as well as a tool of learning — if we are not naturally inclined to use a particular tool for a job, then we will be disadvantaged when it comes to what we get out of whatever we’re applying our tool to. In other words, if we are not proficient with a language when it comes to certain linguistic tasks, then in theory we will not be able to grasp as much of the conceptual content being given to us in that language for that task. Cummins (1979) makes the distinction between what he calls BICS, or Basic Interpersonal Communication Skills, and CALP, or Cognitive / Academic Language Proficiency, where the former denotes skill in language used in a social setting, and the latter refers to skill in language used in an academic setting — Second language acquisition seems to often end up splitting these realms almost completely: What Cummins draws attention to is how learners of minority language backgrounds in English classrooms often have identifiably lower academic achievement than their classmates who speak English natively. There are of course many factors that one would have to consider in doing this kind of analysis, for example cultural differences if the instructor is a native English speaker, but the main issue that seems to keep being isolated through research, according to Cummins, is that it is more difficult for these students simply because they are trying to learn in a language for which they have developed skills in pedagogical / didactic speech, rather than through the proper social medium that’s necessary for a more real linguistic development. As a side note, with this in view, it becomes easy to see how English can easily become disjointed internationally if people who have to learn it for participation in a particular field only learn the associated language of that field, thus developing ‘bad fruit’ of language which will inevitably be passed around to peers / down to future generations.</p>
    <h3>functional limits to internationalization in programming</h3>
    <p>There have been many attempts to ‘internationalize’ programming. For this section of the paper we will focus on two main approaches, an ‘international approach’, where a language specification is published in multiple human languages, and a ‘local approach’, where a whole new programming language is developed for non-English speakers.</p>
    <p>One of the first major attempts at the internationalization of Computer Science was taken during the development of the infamous ALGOL 68 programming language. (ALGOL 68, n.d.) The ALGOL line of programming languages are famous for their influence on the development of other languages which succeeded them and copied elements of their syntax while often simplifying and narrowing their targeted use cases. (O'Hearn & Tennent, 1996) ALGOL 68 was developed, as one might guess, in 1968, and came after the quite successful ALGOL 60. It added many new features to the language specification, and was not only unique for its comprehensiveness and complexity; it was the first to have its standard published in many natural languages, allowing for the creation of compilers which could work with ALGOL 68 code written using keywords taken from languages other than English.</p>
    <div class='content-image-box'>
        <img src='../images/computing-and-english/algol.png'>
    </div>
    <p class='image-note content-image-box'>Figure 4.<br>Equivalent ALGOL 68 in English and German<br>Note. [Screengrab] from</p>
    <a href='https://en.wikipedia.org/wiki/ALGOL_68' class='image-note-link'>https://en.wikipedia.org/wiki/ALGOL_68.</a>
    <p>Today, many people have never heard of ALGOL 68 — the problem which kept ALGOL 68 from becoming ubiquitous was not this altruistic feature; the language specification itself, independent of whatever language it might have been written in, was famously bloated and overengineered. While some compilers were made, it was quickly relegated to a position as an academic tool as opposed to a language used for actual software development, as there was simply no way to make any functional use out of it. Languages like C and Pascal which came after and were successful, were not really intended to be an international touchstone for the apex of programming language theory like ALGOL 68; they were meant to be tools, and in a discipline which by that point was thoroughly English speaking, they were developed in the English language with no regard for the internationalization of ALGOL 68.</p>
    <p>There have been other languages developed not for international audiences, but for localized audiences that are not English speaking, or that aren’t predominantly English speaking. You can look at the simple compiled list on Wikipedia to get a sense of just how many there are; (Non-English based programming languages, 2021) it’s also a good reflection of actually how many programming languages there are in existence versus how many are regularly used. There may be nothing inherently wrong with these functionally; the core problem is that when a programming language is built on top of a natural language, it sort of becomes a closed loop in that language — you’d either have to invest in a cross-compiler (which we will look at later) or accept the fact that keywords, being fixed to the languages they’re derived from, are also fixed to the programming language itself. This essentially means that once something is developed in a non-English based programming language, it and the intellectual capital surrounding the development of that piece of software are fixed to that language — you basically trade massive amounts of internationalization and access to global resources for local affinity, which in the age of international markets and instant communication is usually not worth it. Thus, programming languages’ inherent lack of interoperability with each other means that when there’s competition between them for use by programmers, those that exist in bigger markets will draw more programmers and thus more resources to them, making them more and more usable, and therefore more and more popular. Thus the solution at this level is, sadly, “Get used to English”. Not quite satisfactory.</p>
    <h3>learing cs as a nonnative english speaker</h3>
    <p>One of the most important, and usually final things learned in language learning is what is sometimes called the ‘idiom’. This general term simply refers to the collection of ‘idioms’, or idiomatic expressions which have a specific meaning in a language or dialect that is not obvious at the surface level. For example, the English idiom about having “butterflies in your stomach”, doesn’t at all refer to there actually being any butterflies in anyone's stomach. The point being, the idiom is a cultural, and not rational aspect of language. It is simply something that has to be learned in and of itself, for both young children learning the language natively and second language learners alike. It should come as no surprise that computing terminology is full of the English idiom. Examples include ‘bubble sort’, or ‘shell’, etc.</p>
    <p>It’s this sort of thing we need to keep in mind when thinking about how much more difficult it might be for non-native English speakers to learn CS concepts. Guo (2018) investigated the barriers non-native English speakers face when learning programming by collecting survey responses online. Of the around 850 respondents received from his survey, 96% reported reading their programming materials in English, despite only 22% of respondents being native English speakers. Firstly, the most common issue reported by respondents was that it is simply harder to read auxiliary materials such as textbooks, online tutorials, and online documentation because of their English. This makes sense, as these are the sorts of things which demand the highest level of and most amount of English (And thereby the most amount of the ‘idiom’) in the process of programming. Respondents also tended to talk about problems in technical communication with “listening and speaking”, as well as problems reading code. Overall, while there were both problems with reading code as well as problems with technical communication, the latter was more prevalent.</p>
    <p>Idris & Ammar (2018) provide an instructive study on the effects of English proficiency on programming learning. They took survey results from 218 students and instructors from the University of Benghazi, who spoke Arabic, not English natively. The responses they received were quite telling: ~ 40% of the students that participated in the study said that English was the primary limiting factor in their way to being successful in programming class. This fraction, if we take it to be a common average across the board for non-native English speakers learning programming, is very high, and should provide ample motivation for trying to find a solution to this problem. Idris & Ammar also point to other studies which highlight an important point — that verbose yet shrunken English error messages can be very hard to read for people who need that English to be as clear as possible.</p>
    <p>One final account that is particularly interesting is a personal anecdote from Russian programming blogger Artem Chistyakov. (2017) In a post called The Language of Programming, he details his youth experience with learning programming when the English keywords of the programming languages he was learning were not only in a different language to his native Russian, but in a different alphabet. He gives a great analogy where he equates his experience learning latin alphabetic English programming keywords as a Russian with native English speaking programmers having to memorize emoji instead of English keywords — the point being that rather than having to memorize exotic sounding names in a vacuum like nonnative English speakers might, we can actually understand them, and therefore reason more quickly about and with them. He also talks about how common functions like getchr() and clrscr() provide hints to their meaning through their names only to the native English speaker. Sometimes, we native English speakers need to be reminded of how blessed we are in this context.</p>
    <h3>a modest partial solution</h3>
    <p>In this section, I will convey what I think to be the sort of simple, yet intuitive partial solution that could really help nonative or non speakers of English with learning mainstream programming. This would not help students of programming with the overwhelming proportion of online materials written in English, nor could it help with technical communication in the workplace. But what it could do is help those for whom English is not native recognize and learn programming elements, such as keywords, function and method names, etc. which would otherwise be ‘locked behind’ English.</p>
    <p>What I am essentially proposing is a system where a source to source compiler (Source-to-source compiler, 2021) is set up on a website which recognizes where a user is in the world and changes the language of the site accordingly, making it obvious and easy how to change the language manually if necessary. This source to source compiler would work on top of a massive set of hashtables, each of which represents a relationship between two languages — The language of the source code of the author and English. There would be separate hashtables per ‘foreign’ language for keywords, method names, etc., and for each table, the keys would represent the possible names of that element in the ‘foreign’ language, while the value associated with a key would represent the legal name of the element in English. This gives us a surjective relationship between the two language name sets, where the idiom of the ‘foreign’ language being inputted can be preserved and any valid translation, determined by the users of the site, would still map to its associated English name and allow the programmer to compile a program in a major English programming language without actually having to think in English. Keys which map to English names would be added to the site via a wiki system, meaning many more languages and idiomatic names could be added to the site than if an internal team tried to cover the breadth of possible name translations by themselves. There would of course be a team of editors in place to protect against vandalism, but in reality, this wouldn’t even be that much of a problem, since the system isn’t mapping inputs to multiple outputs — if an internet troll makes it so that ‘gelato’, in the Italian map for Java connects to ‘while’ in the English, that would only be a problem if an author of Italian Java accidentally wrote ‘gelato’ in place of their word for ‘while’. Quite the typo that would be!</p>
    <p>The site could give the user back the source code in English, or it could give back machine code by compiling the source code server-side. It is recognized that this would be much, much slower than if the program was just compiled on the host machine without using the internet, but with how fast the internet can be today, and assuming that this would probably be more of an educational tool than a large scale project manager, I think the sacrifice is probably worth it — even if the compilation speed ‘round trip’ is hundreds of times slower than if it was done locally, if the speed was negligible to begin with, the ‘slow’ speed may still seem very fast.</p>
    <div class='content-image-box'>
        <img src='../images/computing-and-english/keyword-mapping.png'>
    </div>
    <p class='image-note content-image-box'>Figure 5.<br>Table of Italian to English hashes for the Java while keyword</p>
    <div class='content-image-box'>
        <img src='../images/computing-and-english/mapping-hierarchy.png'>
    </div>
    <p class='image-note content-image-box'>Figure 6.<br>Structure of system of tables, with example translations</p>
    <h3>conclusion</h3>
    <p>In conclusion, we’ve determined that the dominance of English over Computer Science represents a significant hurdle for nonnative English speakers and nonspeakers of English. The dominance of English over all world communication is a linguistic reality not without precedent, but perhaps without parallel. In today’s world, if one wants to be a formidable programmer, one has to know an at least operable amount of the lingua franca. However, I take the personal stance that Computer Scientists, and lazy Anglophones are not doing enough to address the lack of linguistic diversity, and the lack of opportunity along these lines in Computing. Computer Scientists are famous for their ability to bootstrap their way out of ‘toolset’ problems, so why haven’t we tried harder to work with the majority of the world’s population here?</p>
    <h3>references</h3>
    <p>Abbott, R. J. (1983). Program Design by Informal English Descriptions. Communications of the ACM, 26(11).<a href='https://doi.org/10.1145/358024'>https://doi.org/10.1145/358024</a></p>
    <p>Anderson, M. J. (2015). Census and Industrial America. In The American Census: A Social History (2nd ed., p. 102). essay, Yale Univ. Press.</p>
    <p>Bakker, P. (1994). Pidgins. In J. Arends, P. Muysken, & N. Smith (Eds.), Pidgins and creoles: An introduction (Vol. 15, pp. 28–30). essay, John Benjamins.</p>
    <p>Barbin, E., Borowczyk, J., Chabert, J.-L., Guillemot, M., & Pajus, A. (1999). Algorithms for Arithmetic Operations. In J.-L. Chabert (Ed.), A history of algorithms: From the pebble to the microchip. essay, Springer.</p>
    <p>Ben Idris, M., & Ammar, H. (2018). The correlation between Arabic student’s English proficiency and their computer programming ability at the university level. SSRN Electronic Journal.<a href='https://doi.org/10.2139/ssrn.3466182'>https://doi.org/10.2139/ssrn.3466182</a></p>
    <p>Chistyakov, A. (2017, June 28). The language of programming. temochka.com/blog. Retrieved December 8, 2021, from <a href='https://temochka.com/blog/posts/2017/06/28/the-language-of-programming.html'>https://temochka.com/blog/posts/2017/06/28/the-language-of-programming.html.</a></p>
    <p>Contributors to Wikimedia. (2020, September 10). Treaty of Versailles Protocol. Wikisource, the free online library. Retrieved December 6, 2021, from <a href='https://en.wikisource.org/wiki/Treaty_of_Versailles/Protocol'>https://en.wikisource.org/wiki/Treaty_of_Versailles/Protocol.</a></p>
    <p>Coupland, N., & Swaan, A. de. (2013). Language Systems. In The handbook of language and globalization essay, Wiley-Blackwell.</p>
    <p>Cruz, F. da. (2001, January). Herman Hollerith. columbia.edu. Retrieved December 6, 2021, from<a href='http://www.columbia.edu/cu/computinghistory/hollerith.html'>http://www.columbia.edu/cu/computinghistory/hollerith.html.</a></p>
    <p>Crystal, D. (n.d.). Two thousand million? Cambridge. Retrieved December 7, 2021, from <a href='https://www.cambridge.org/core/services/aop-cambridge-core/content/view/68BFD87E5C867F7C3C47FD0749C7D417/S0266078408000023a.pdf/two_thousand_million.pdf'>https://www.cambridge.org/core/services/aop-cambridge-core/content/view/68BFD87E5C867F7C3C47FD0749C7D417/S0266078408000023a.pdf/two_thousand_million.pdf.</a></p>
    <p>Cummins, J. (1979). Linguistic interdependence and the educational development of bilingual children. Review of Educational Research, 49(2), 222–251. https://doi.org/10.3102/00346543049002222</p>
    <p>Encyclopædia Britannica, inc. (1998, July 20). ASCII. Encyclopædia Britannica. Retrieved December 5, 2021, from <a href='https://www.britannica.com/topic/ASCII'>https://www.britannica.com/topic/ASCII.</a></p>
    <p>Fyfe, D. (2019, April 16). Unicode. Medium. Retrieved December 5, 2021, from <a href='https://medium.com/@danfyfe/unicode-7626c437e62b'>https://medium.com/@danfyfe/unicode-7626c437e62b.</a></p>
    <p>Hackett, J. A. (2008). Chapter 4: Phoenician and Punic. In R. D. Woodard (Ed.), The ancient languages of Syria-palestine and Arabia (pp. 82–103). essay, Cambridge University Press.</p>
    <p>IBM. (n.d.). Unicode Character Encoding. IBM. Retrieved December 5, 2021, from <a href='https://www.ibm.com/docs/en/db2/11.5?topic=support-unicode-character-encoding'>https://www.ibm.com/docs/en/db2/11.5?topic=support-unicode-character-encoding.</a></p>
    <p>Ikenberry, G. J. (2004). Liberal hegemony and the future of American postwar order. In T. V. Paul & J. A. Hall (Eds.), International Order and the future of world politics (p. 123). essay, Cambridge University Press.</p>
    <p>Kaplan, R. B. (2001). English — The Accidental Language of Science? In U. Ammon (Ed.), The dominance of English as a language of science: Effects on other languages and language communities (pp. 3–26). essay, Mouton de Gruyter.</p>
    <p>Kerschen, K., & Matrinez, J. C. (n.d.). Children vs. Adults – Who Wins the Second Language Acquisition Match? psu.edu. Retrieved December 7, 2021, from https://sites.psu.edu/bilingualismmatters/children-vs-adults-who-wins-the-second-language-acquisition-match/.</p>
    <p>Language and Diplomacy. Naked Translation. (2011, April 29). Retrieved December 6, 2021, from <a href='https://web.archive.org/web/20110721070018/http://www.nakedtranslations.com/en/2004/language-and-diplomacy/'>https://web.archive.org/web/20110721070018/http://www.nakedtranslations.com/en/2004/language-and-diplomacy/.</a></p>
    <p>McLuhan, M., & Logan, R. K. (1977). Alphabet, Mother of Invention. ETC: A Review of General Semantics, 34(4), 373–383.</p>
    <p>Mufwene, S. S. (2010, August 12). Lingua franca. Encyclopædia Britannica. Retrieved December 4, 2021, from <a href='https://www.britannica.com/topic/lingua-franca'>https://www.britannica.com/topic/lingua-franca.</a></p>
    <p>O'Hearn, P. W., & Tennent, R. D. (1996, September). Algol-like languages, introduction. Algol-Like Languages, Introduction. Retrieved December 8, 2021, from <a href='https://web.archive.org/web/20111114122103/http://www.eecs.qmul.ac.uk/~ohearn/Algol/intro.html'>https://web.archive.org/web/20111114122103/http://www.eecs.qmul.ac.uk/~ohearn/Algol/intro.html.</a></p>
    <p>O'Regan, G. (2021). A brief history of computing. Springer.</p>
    <p>Porter, A. N. (2001). Introduction: Britain and the Empire in the Nineteenth Century. In A. N. Porter (Ed.), The Oxford History of the British Empire: Volume III, the nineteenth century. (Vol. 3, pp. 1–28). essay, Oxford University Press.</p>
    <p>Quirk, R., & Wrenn, C. L. (1957). Inflexions. In An old english grammar (pp. 19–40). essay, Methuen.</p>
    <p>Samarin, W. J. (1977). Lingua Francas of the World. In J. A. Fishman (Ed.), Readings in the sociology of language (pp. 660–663). essay, Mouton.</p>
    <p>Schmidhuber, J. (n.d.). 1931: Theoretical Computer Science & AI theory founded by Goedel. people.idsia.ch. Retrieved December 1, 2021, from <a href='https://people.idsia.ch/~juergen/goedel-1931-founder-theoretical-computer-science-AI.html#GOD'>https://people.idsia.ch/~juergen/goedel-1931-founder-theoretical-computer-science-AI.html#GOD.</a></p>
    <p>Solodow, J. B. (2010). Introduction: English Is Not a Cousin to the Romance Languages, But . . . In Latin alive: The survival of Latin in English and romance languages (pp. 1–8). essay, Cambridge University Press.</p>
    <p>The Ada Programming Language. University of Michigan. (n.d.). Retrieved December 3, 2021, from <a href='https://web.archive.org/web/20160522063844/http://groups.engin.umd.umich.edu/CIS/course.des/cis400/ada/ada.html'>https://web.archive.org/web/20160522063844/http://groups.engin.umd.umich.edu/CIS/course.des/cis400/ada/ada.html.</a></p>
    <p>Turing, A. M. (1937). On computable numbers, with an application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, s2-42(1), 230–265. https://doi.org/10.1112/plms/s2-42.1.230</p>
    <p>Uma, V., & Suseela, V. J. (2014). Unicode Applications in the Digital Libraries of India. In Current practices in academic librarianship (pp. 40–54). essay, Allied Publishers Pvt. Ltd.</p>
    <p>University of Surrey. (n.d.). Loss of inflection. UK Research and Innovation. Retrieved from <a href='https://gtr.ukri.org/projects?ref=AH%2FN00163X%2F1'>https://gtr.ukri.org/projects?ref=AH%2FN00163X%2F1.</a></p>
    <p>US Census Bureau Publications - census of Population and Housing. United States Census Bureau. (2011, August 19). Retrieved December 6, 2021, from <a href='https://www.census.gov/prod/www/decennial.html'>https://www.census.gov/prod/www/decennial.html.</a></p>
    <p>Warschauer, M. (2000). Language, Identity, and the Internet. In B. E. Kolko, L. Nakamura, & G. B. Rodman (Eds.), Race in cyberspace (pp. 155–158). essay, Routledge.</p>
    <p>Weber, G. (2004, April 28). TOP LANGUAGES: The World's 10 most influential Languages. andaman.org. Retrieved December 5, 2021, from <a href='https://web.archive.org/web/20080312042140/http://www.andaman.org/BOOK/reprints/weber/rep-weber.htm'>https://web.archive.org/web/20080312042140/http://www.andaman.org/BOOK/reprints/weber/rep-weber.htm.</a></p>
    <p>What is Unicode? webfocusinfocenter.informationbuilders. (n.d.). Retrieved December 5, 2021, from <a href='https://webfocusinfocenter.informationbuilders.com/wfappent/TL2s/TL_nls/source/01_nls_7720.htm'>https://webfocusinfocenter.informationbuilders.com/wfappent/TL2s/TL_nls/source/01_nls_7720.htm.</a></p>
    <p>Wigderson, A. (2019). Prelude: Computation, undecidability, and limits to mathematical knowledge. In Mathematics and computation: A theory revolutionizing technology and science (p. 12). essay, Princeton University Press.</p>
    <p>Wikimedia Foundation. (n.d.). Algol 68. Wikipedia. Retrieved December 8, 2021, from <a href='https://en.wikipedia.org/wiki/ALGOL_68'>https://en.wikipedia.org/wiki/ALGOL_68.</a></p>
    <p>Wikimedia Foundation. (2021, November 25). ENIAC. Wikipedia. Retrieved December 6, 2021, from <a href='https://en.wikipedia.org/wiki/ENIAC'>https://en.wikipedia.org/wiki/ENIAC.</a></p>
    <p>Wikimedia Foundation. (2021, November 29). Lambda calculus. Wikipedia. Retrieved December 6, 2021, from <a href='https://en.wikipedia.org/wiki/Lambda_calculus'>https://en.wikipedia.org/wiki/Lambda_calculus.</a></p>
    <p>Wikimedia Foundation. (2021, November 30). Lisp (programming language). Wikipedia. Retrieved December 6, 2021, from <a href='https://en.wikipedia.org/wiki/Lisp_(programming_language)'>https://en.wikipedia.org/wiki/Lisp_(programming_language).</a></p>
    <p>Wikimedia Foundation. (2021, November 28). Non-english-based programming languages. Wikipedia. Retrieved December 8, 2021, from <a href='https://en.wikipedia.org/wiki/Non-English-based_programming_languages'>https://en.wikipedia.org/wiki/Non-English-based_programming_languages.</a></p>
    <p>Wikimedia Foundation. (2021, November 28). Source-to-source compiler. Wikipedia. Retrieved December 8, 2021, from <a href='https://en.wikipedia.org/wiki/Source-to-source_compiler'>https://en.wikipedia.org/wiki/Source-to-source_compiler.</a></p>
    <p>Woods, C. (2010). The Earliest Mesopotamian Writing. In C. Woods, G. Emberling, & E. Teeter (Eds.), Visible language: Inventions of writing in the ancient Middle East and beyond (pp. 33–85). essay, Oriental Institute of the University of Chicago.</p>
    <br><a href='../index.html'>back</a>
</body>
</html>